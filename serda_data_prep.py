""""
@Author:        Bo Molenaar
@Date:          1 February 2023

@Last edited:    17 March 2023

This script processes SERDA v1 data after they have been organised into task directories.
It requires a dict from serda_data_sel.py with recording IDs and paths to the audio and log for that recording.

Audio files for word tasks are split on single words with added word ID tag.

"""

#!/usr/bin/python3
# -*- coding: utf-8 -*-

import os
import argparse
import shutil
import pathlib
from subprocess import run
import pandas as pd


parser = argparse.ArgumentParser()
parser.add_argument('-c', '--clean', help = "Flag specifying whether you want to generate new directories. Default=False", action = 'store_true')
parser.add_argument('audio_path', help = "Path to audio processing and storing dir")
parser.add_argument('log_path', help = "Path to log processing and storing dir")
parser.add_argument('prompts_source', help = "Path to story prompts")
parser.add_argument('prompt_path', help = "Path to prompt processing and storing dir")
parser.add_argument('AO_source', help = "Path to ASR output for story task recs")
args = parser.parse_args()


def segment_words(full_rec_path, rec_segments_path, words_dict):
    """
    Takes a dict for 1 word task with items 'prompt_id': ('prompt_id', 'segment_start', 'segment_end').
    Then recalculates start time as word appearance (= prev word end + 1323ms).
    Finally the timestamps are used to create an audio file for each segment.
    """
    for prompt_id, (start_time, end_time) in words_dict:
        segment_path = f"{full_rec_path[:-4]}_{prompt_id}.wav"
        
        # TODO:
        # make an exception case for the first word, where two files are generated
        # one file starts from the beginning of the recording
        # the other file starts from the first word start click event

        # TODO:
        # all other cases will have their segments time based on the end of the previous word, so start_time is not relevant
        
        soxcommand = f"sox {full_rec_path} {segment_path} trim {start_time} ={end_time} pad 0.3 0.3"
        run(soxcommand, check=True, shell=True)
        if prompt_id == 1:
            soxcommand = f"sox {full_rec_path} {segment_path} trim {start_time} ={end_time} pad 0.3 0.3"
            run(soxcommand, check=True, shell=True)



def segment_story(full_rec_path, rec_segments_path, rec_WX_AO):
    """
    Takes a story audio file and matches it to its story prompt.
    Then, bootstrapped segments generated by WhisperX ASR are searched 
    to identify start/end timestamps in the audio for each line in the prompt.
    
    """
    # TODO:
    # stories will be segmented based on WhisperX ASR output
    # this means we need timestamps for each segment in the JSON
    # --> match prompt line to id in JSON

def prepare_data(full_dict, audio_path, log_path, prompts_source, prompt_path, clean_dirs):
    """
    docstring
    """

    words_dir = "words"
    stories_dir = "stories"
    segments_dir = "segments"

    audio_words_path = os.path.join(audio_path, words_dir, "full")
    audio_stories_path = os.path.join(audio_path, stories_dir)
    log_words_path = os.path.join(log_path, words_dir)
    log_stories_path = os.path.join(log_path, stories_dir)
    prompt_words_path = os.path.join(prompt_path, words_dir)
    prompt_stories_path = os.path.join(prompt_path, stories_dir)
    words_segments_path = os.path.join(audio_path, words_dir, segments_dir)

    #  remove working directories for audio and logs and create fresh ones
    if clean_dirs:
        dir_lst = [prompt_words_path, prompt_stories_path,
                   words_segments_path]
        for mydir in dir_lst:
            shutil.rmtree(mydir)
            pathlib.Path(mydir).mkdir(parents=True, exist_ok=True)


    for rec_id, (audio, log) in full_dict.items():
        # handle story tasks
        if "story" in rec_id:      
            # generate prompt file
            storynum = rec_id.split("-")[1].replace("_", "")
            prompt = ""
            infile = os.path.join(prompts_source, f"{storynum}_cut.txt")
            outfile = os.path.join(prompt_stories_path, f"{rec_id}.prompt")
            with open(infile, "r", encoding="utf-8") as prompt_in, open(outfile, "w", encoding="utf-8") as prompt_out:
                for line in prompt_in.readlines():
                    prompt += line
                prompt_out.write(prompt)

        # handle word tasks
        elif "words" in rec_id:
            word_segments = {}
            # load logfile to get the word timestamps
            log_data = pd.read_csv(log, delimiter=";", index_col="user_id")

            for speaker_id, row in log_data.iterrows():
                word_segments[row['prompt_id']] = (row['start_speak'], row['stop_speak'])
                prompt = str(row['prompt'])
                prompt_id = row['prompt_id']
                outfile = os.path.join(prompt_words_path, f"{rec_id}_{prompt_id}.prompt")
                with open(outfile, "w", encoding="utf-8") as prompt_out:
                    prompt_out.write(prompt)


            # segment_words(audio, word_segments)

    # for item in sorted(list(word_segments.items()))[:10]:
    #     print(item)

if __name__ == "__main__":
    prepare_data(args.full_dict, args.audio_path, args.log_path, args.prompts_source, args.prompt_path, args.clean_dirs)
